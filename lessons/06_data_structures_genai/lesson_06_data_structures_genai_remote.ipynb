{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6 â€“ Lists, Tuples & Dictionaries for GenAI\n",
    "\n",
    "In this lesson, you'll learn about Python's three most important data structures and how they're used in AI and machine learning applications.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand **lists**, **tuples**, and **dictionaries**\n",
    "- Learn when to use each data structure\n",
    "- Apply these concepts to GenAI scenarios\n",
    "- Practice with real-world AI examples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lists - Dynamic Collections\n",
    "\n",
    "**Lists** are ordered, mutable collections that can store multiple items of any type.\n",
    "\n",
    "### Basic List Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists\n",
    "ai_models = [\"GPT-4\", \"Claude\", \"Gemini\", \"LLaMA\"]\n",
    "model_scores = [95, 92, 88, 85]\n",
    "mixed_data = [\"GPT-4\", 95, True, 3.14]\n",
    "\n",
    "print(\"AI Models:\", ai_models)\n",
    "print(\"Scores:\", model_scores)\n",
    "print(\"Mixed data:\", mixed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing list elements (indexing)\n",
    "print(\"First model:\", ai_models[0])\n",
    "print(\"Last model:\", ai_models[-1])\n",
    "print(\"Best score:\", max(model_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying lists\n",
    "ai_models.append(\"Mistral\")  # Add to end\n",
    "ai_models.insert(1, \"Bard\")  # Insert at position\n",
    "ai_models.remove(\"Bard\")     # Remove by value\n",
    "\n",
    "print(\"Updated models:\", ai_models)\n",
    "print(\"Number of models:\", len(ai_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Example: Processing User Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world GenAI scenario: Managing conversation history\n",
    "conversation_history = []\n",
    "\n",
    "# Adding messages to conversation one by one\n",
    "conversation_history.append({\"role\": \"user\", \"content\": \"What is machine learning?\"})\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": \"Machine learning is a subset of AI that enables computers to learn from data.\"})\n",
    "conversation_history.append({\"role\": \"user\", \"content\": \"Explain neural networks\"})\n",
    "conversation_history.append({\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks.\"})\n",
    "\n",
    "print(f\"Conversation has {len(conversation_history)} messages\")\n",
    "print(\"Last message:\", conversation_history[-1])\n",
    "print(\"First message:\", conversation_history[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic List Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing AI prompts step by step\n",
    "raw_prompt1 = \"  EXPLAIN AI  \"\n",
    "raw_prompt2 = \"  what is ML?  \"\n",
    "raw_prompt3 = \"  DEFINE NLP  \"\n",
    "\n",
    "# Clean prompts one by one\n",
    "clean_prompt1 = raw_prompt1.strip().lower()\n",
    "clean_prompt2 = raw_prompt2.strip().lower()\n",
    "clean_prompt3 = raw_prompt3.strip().lower()\n",
    "\n",
    "print(\"Original:\", raw_prompt1)\n",
    "print(\"Cleaned:\", clean_prompt1)\n",
    "print(\"Length:\", len(clean_prompt1))\n",
    "\n",
    "# Create a list of cleaned prompts\n",
    "clean_prompts = [clean_prompt1, clean_prompt2, clean_prompt3]\n",
    "print(\"All cleaned prompts:\", clean_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Tuples - Immutable Collections\n",
    "\n",
    "**Tuples** are ordered, immutable collections. Perfect for data that shouldn't change.\n",
    "\n",
    "### Basic Tuple Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tuples\n",
    "model_info = (\"GPT-4\", \"OpenAI\", \"2023\", 1.76e12)  # (name, company, year, parameters)\n",
    "coordinates = (10.5, 20.3)  # x, y coordinates\n",
    "rgb_color = (255, 128, 0)   # Red, Green, Blue values\n",
    "\n",
    "print(\"Model info:\", model_info)\n",
    "print(\"Type:\", type(model_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing tuple elements\n",
    "model_name = model_info[0]\n",
    "company = model_info[1]\n",
    "param_count = model_info[3]\n",
    "\n",
    "print(f\"{model_name} by {company} has {param_count:.1e} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuple unpacking (very useful!)\n",
    "name, company, year, parameters = model_info\n",
    "print(f\"Unpacked: {name} was released by {company} in {year}\")\n",
    "\n",
    "# Multiple assignment using tuples\n",
    "x, y = coordinates\n",
    "print(f\"Position: ({x}, {y})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Example: Model Configuration and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model configurations (immutable settings)\n",
    "gpt35_config = (\"gpt-3.5-turbo\", 0.7, 150, 0.01)  # (model, temperature, max_tokens, cost_per_1k)\n",
    "gpt4_config = (\"gpt-4\", 0.5, 200, 0.03)\n",
    "claude_config = (\"claude-3\", 0.8, 180, 0.02)\n",
    "\n",
    "# Accessing tuple elements\n",
    "model_name, temperature, max_tokens, cost = gpt4_config\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max tokens: {max_tokens}\")\n",
    "print(f\"Cost per 1k tokens: ${cost}\")\n",
    "\n",
    "# Create a list of all configs\n",
    "all_configs = [gpt35_config, gpt4_config, claude_config]\n",
    "print(f\"\\nTotal models available: {len(all_configs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returning multiple values (using tuples)\n",
    "def analyze_text(text):\n",
    "    word_count = len(text.split())\n",
    "    char_count = len(text)\n",
    "    avg_word_length = char_count / word_count if word_count > 0 else 0\n",
    "    return word_count, char_count, avg_word_length\n",
    "\n",
    "# Example: Analyzing AI-generated text\n",
    "ai_response = \"Artificial intelligence is transforming how we interact with technology.\"\n",
    "words, chars, avg_len = analyze_text(ai_response)\n",
    "\n",
    "print(f\"Text analysis: {words} words, {chars} characters, avg length: {avg_len:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dictionaries - Key-Value Mappings\n",
    "\n",
    "**Dictionaries** store data as key-value pairs. Essential for structured data in AI applications.\n",
    "\n",
    "### Basic Dictionary Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionaries\n",
    "ai_model = {\n",
    "    \"name\": \"GPT-4\",\n",
    "    \"company\": \"OpenAI\",\n",
    "    \"type\": \"Language Model\",\n",
    "    \"parameters\": 1760000000000,\n",
    "    \"multimodal\": True\n",
    "}\n",
    "\n",
    "print(\"AI Model:\", ai_model)\n",
    "print(\"Model name:\", ai_model[\"name\"])\n",
    "print(\"Is multimodal:\", ai_model[\"multimodal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding and modifying dictionary data\n",
    "ai_model[\"release_year\"] = 2023\n",
    "ai_model[\"parameters\"] = \"1.76 trillion\"  # More readable format\n",
    "\n",
    "print(\"Updated model:\")\n",
    "print(f\"  name: {ai_model['name']}\")\n",
    "print(f\"  company: {ai_model['company']}\")\n",
    "print(f\"  type: {ai_model['type']}\")\n",
    "print(f\"  parameters: {ai_model['parameters']}\")\n",
    "print(f\"  multimodal: {ai_model['multimodal']}\")\n",
    "print(f\"  release_year: {ai_model['release_year']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe access with .get() method\n",
    "cost = ai_model.get(\"cost_per_token\", \"Not specified\")\n",
    "print(f\"Cost: {cost}\")\n",
    "\n",
    "# Check if key exists\n",
    "if \"multimodal\" in ai_model:\n",
    "    print(f\"This model supports multimodal: {ai_model['multimodal']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Example: Managing API Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating API response from an AI service\n",
    "api_response = {\n",
    "    \"id\": \"chatcmpl-123\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"Machine learning is a powerful subset of artificial intelligence.\"\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 12,\n",
    "        \"completion_tokens\": 15,\n",
    "        \"total_tokens\": 27\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extracting information from nested dictionaries\n",
    "response_text = api_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "total_tokens = api_response[\"usage\"][\"total_tokens\"]\n",
    "model_used = api_response[\"model\"]\n",
    "\n",
    "print(f\"AI Response: {response_text}\")\n",
    "print(f\"Tokens used: {total_tokens} (Model: {model_used})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GenAI Example: Prompt Templates and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI prompt templates\n",
    "prompt_templates = {\n",
    "    \"explain\": \"Explain {topic} in simple terms for a beginner.\",\n",
    "    \"code\": \"Write Python code to {task}. Include comments.\",\n",
    "    \"debug\": \"Help me debug this Python code: {code}\"\n",
    "}\n",
    "\n",
    "# Using templates\n",
    "topic = \"neural networks\"\n",
    "explain_template = prompt_templates[\"explain\"]\n",
    "explain_prompt = explain_template.format(topic=topic)\n",
    "print(f\"Template: {explain_template}\")\n",
    "print(f\"Generated prompt: {explain_prompt}\")\n",
    "\n",
    "# AI model settings\n",
    "ai_settings = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 150,\n",
    "    \"top_p\": 0.9,\n",
    "    \"frequency_penalty\": 0.1\n",
    "}\n",
    "\n",
    "print(\"\\nAI Configuration:\")\n",
    "print(f\"  temperature: {ai_settings['temperature']}\")\n",
    "print(f\"  max_tokens: {ai_settings['max_tokens']}\")\n",
    "print(f\"  top_p: {ai_settings['top_p']}\")\n",
    "print(f\"  frequency_penalty: {ai_settings['frequency_penalty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Combining Data Structures\n",
    "\n",
    "Real GenAI applications often combine lists, tuples, and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI model information using combined data structures\n",
    "gpt4_model = {\n",
    "    \"name\": \"GPT-4\",\n",
    "    \"company\": \"OpenAI\",\n",
    "    \"capabilities\": [\"text\", \"image\", \"code\"],\n",
    "    \"pricing\": (0.03, 0.06),  # (input_cost, output_cost) per 1k tokens\n",
    "    \"max_context\": 8192\n",
    "}\n",
    "\n",
    "claude_model = {\n",
    "    \"name\": \"Claude-3\",\n",
    "    \"company\": \"Anthropic\",\n",
    "    \"capabilities\": [\"text\", \"image\", \"analysis\"],\n",
    "    \"pricing\": (0.015, 0.075),\n",
    "    \"max_context\": 200000\n",
    "}\n",
    "\n",
    "# Accessing nested data\n",
    "print(f\"Model: {gpt4_model['name']}\")\n",
    "print(f\"Company: {gpt4_model['company']}\")\n",
    "print(f\"First capability: {gpt4_model['capabilities'][0]}\")\n",
    "print(f\"Second capability: {gpt4_model['capabilities'][1]}\")\n",
    "print(f\"Total capabilities: {len(gpt4_model['capabilities'])}\")\n",
    "\n",
    "# Accessing tuple within dictionary\n",
    "input_cost, output_cost = gpt4_model[\"pricing\"]\n",
    "print(f\"Input cost: ${input_cost}/1k tokens\")\n",
    "print(f\"Output cost: ${output_cost}/1k tokens\")\n",
    "\n",
    "# Create a list of models\n",
    "ai_models_db = [gpt4_model, claude_model]\n",
    "print(f\"\\nTotal models in database: {len(ai_models_db)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking model capabilities\n",
    "print(\"Checking which models can process images:\")\n",
    "\n",
    "# Check GPT-4\n",
    "if \"image\" in gpt4_model[\"capabilities\"]:\n",
    "    print(f\"âœ“ {gpt4_model['name']} can process images\")\n",
    "else:\n",
    "    print(f\"âœ— {gpt4_model['name']} cannot process images\")\n",
    "\n",
    "# Check Claude\n",
    "if \"image\" in claude_model[\"capabilities\"]:\n",
    "    print(f\"âœ“ {claude_model['name']} can process images\")\n",
    "else:\n",
    "    print(f\"âœ— {claude_model['name']} cannot process images\")\n",
    "\n",
    "# Compare costs\n",
    "gpt4_input_cost = gpt4_model[\"pricing\"][0]\n",
    "claude_input_cost = claude_model[\"pricing\"][0]\n",
    "\n",
    "print(f\"\\nCost comparison:\")\n",
    "print(f\"GPT-4 input cost: ${gpt4_input_cost}/1k tokens\")\n",
    "print(f\"Claude input cost: ${claude_input_cost}/1k tokens\")\n",
    "\n",
    "if gpt4_input_cost < claude_input_cost:\n",
    "    print(\"GPT-4 is cheaper for input\")\n",
    "else:\n",
    "    print(\"Claude is cheaper for input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Practice Exercises\n",
    "\n",
    "### Exercise 1: Conversation Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a conversation manager\n",
    "# 1. Create an empty list called 'conversation'\n",
    "# 2. Add at least 3 message dictionaries with 'role' and 'content' keys\n",
    "# 3. Print the total number of messages\n",
    "# 4. Print the first and last messages\n",
    "\n",
    "# Your code here:\n",
    "conversation = []\n",
    "\n",
    "# Example: Add your messages like this:\n",
    "# conversation.append({\"role\": \"user\", \"content\": \"Hello AI!\"})\n",
    "# conversation.append({\"role\": \"assistant\", \"content\": \"Hello! How can I help?\"})\n",
    "\n",
    "# Print results\n",
    "# print(f\"Total messages: {len(conversation)}\")\n",
    "# print(f\"First message: {conversation[0]}\")\n",
    "# print(f\"Last message: {conversation[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Model Performance Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a model performance tracker\n",
    "# 1. Create a dictionary with model names as keys\n",
    "# 2. Each value should be a tuple of (accuracy, speed, cost)\n",
    "# 3. Compare models by accessing their data\n",
    "# 4. Find which model has the best accuracy\n",
    "\n",
    "# Example data to use:\n",
    "# \"GPT-4\": (95, 8, 0.03)\n",
    "# \"Claude-3\": (93, 12, 0.015)\n",
    "# \"Gemini\": (88, 15, 0.001)\n",
    "\n",
    "# Your code here:\n",
    "model_performance = {\n",
    "    # Add your data like: \"GPT-4\": (95, 8, 0.03)\n",
    "}\n",
    "\n",
    "# Example of how to access data:\n",
    "# gpt4_data = model_performance[\"GPT-4\"]\n",
    "# accuracy, speed, cost = gpt4_data\n",
    "# print(f\"GPT-4 accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Prompt Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a prompt library system\n",
    "# 1. Create a list of prompt categories: [\"coding\", \"writing\", \"analysis\"]\n",
    "# 2. Create a dictionary where each category maps to a list of prompts\n",
    "# 3. Add at least 2 prompts per category\n",
    "# 4. Access prompts using indexing\n",
    "\n",
    "# Your code here:\n",
    "categories = [\"coding\", \"writing\", \"analysis\"]\n",
    "prompt_library = {\n",
    "    # Add your prompts like:\n",
    "    # \"coding\": [\"Write a Python function\", \"Debug this code\"],\n",
    "    # \"writing\": [\"Write an essay about\", \"Create a summary of\"],\n",
    "    # \"analysis\": [\"Analyze this data\", \"Compare these options\"]\n",
    "}\n",
    "\n",
    "# Example of accessing prompts:\n",
    "# coding_prompts = prompt_library[\"coding\"]\n",
    "# first_coding_prompt = coding_prompts[0]\n",
    "# print(f\"First coding prompt: {first_coding_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Takeaways\n",
    "\n",
    "### When to Use Each Data Structure:\n",
    "\n",
    "| Data Structure | Use When | GenAI Examples |\n",
    "|----------------|----------|----------------|\n",
    "| **List** | Ordered, changeable data | Conversation history, token sequences, model outputs |\n",
    "| **Tuple** | Ordered, unchangeable data | Model configurations, coordinates, API responses |\n",
    "| **Dictionary** | Key-value relationships | API responses, model metadata, user profiles |\n",
    "\n",
    "### Common Patterns in GenAI:\n",
    "- **Lists**: Store sequences of messages, tokens, or results\n",
    "- **Tuples**: Package related data (model settings, coordinates)\n",
    "- **Dictionaries**: Structure complex data (API responses, configurations)\n",
    "\n",
    "### Best Practices:\n",
    "1. Use descriptive variable names\n",
    "2. Choose the right data structure for your use case\n",
    "3. Use `.get()` for safe dictionary access\n",
    "4. Combine structures when needed for complex data\n",
    "5. Consider immutability when data shouldn't change\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "- Practice with real AI API responses\n",
    "- Learn about data processing with pandas\n",
    "- Explore JSON handling for API integration\n",
    "- Study prompt engineering techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
